{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b48958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "IPython.tab_as_tab_everywhere = function(use_tabs) {\n",
       "    if (use_tabs === undefined) {\n",
       "        use_tabs = true; \n",
       "    }\n",
       "\n",
       "    // apply setting to all current CodeMirror instances\n",
       "    IPython.notebook.get_cells().map(\n",
       "        function(c) {  return c.code_mirror.options.indentWithTabs=use_tabs;  }\n",
       "    );\n",
       "    // make sure new CodeMirror instances created in the future also use this setting\n",
       "    CodeMirror.defaults.indentWithTabs=use_tabs;\n",
       "\n",
       "    };\n",
       "\n",
       "IPython.tab_as_tab_everywhere()\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "IPython.tab_as_tab_everywhere = function(use_tabs) {\n",
    "    if (use_tabs === undefined) {\n",
    "        use_tabs = true; \n",
    "    }\n",
    "\n",
    "    // apply setting to all current CodeMirror instances\n",
    "    IPython.notebook.get_cells().map(\n",
    "        function(c) {  return c.code_mirror.options.indentWithTabs=use_tabs;  }\n",
    "    );\n",
    "    // make sure new CodeMirror instances created in the future also use this setting\n",
    "    CodeMirror.defaults.indentWithTabs=use_tabs;\n",
    "\n",
    "    };\n",
    "\n",
    "IPython.tab_as_tab_everywhere()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e679e60f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tribe/mabhishe/myspace/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/tribe/mabhishe/myspace/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset loaded from pickle at output_dataset.pkl\n",
      "            Experiment Name     Device Name  \\\n",
      "0  Experiment_1_A_192_0_2_1  petsafe_feeder   \n",
      "1  Experiment_1_A_192_0_2_1  petsafe_feeder   \n",
      "2  Experiment_1_A_192_0_2_1  petsafe_feeder   \n",
      "3  Experiment_1_A_192_0_2_1  petsafe_feeder   \n",
      "4  Experiment_1_A_192_0_2_1  petsafe_feeder   \n",
      "\n",
      "                                         Packet JSON  \n",
      "0  {\"_index\": \"packets-2025-05-01\", \"_type\": \"pca...  \n",
      "1  {\"_index\": \"packets-2025-05-01\", \"_type\": \"pca...  \n",
      "2  {\"_index\": \"packets-2025-05-01\", \"_type\": \"pca...  \n",
      "3  {\"_index\": \"packets-2025-05-01\", \"_type\": \"pca...  \n",
      "4  {\"_index\": \"packets-2025-05-01\", \"_type\": \"pca...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import subprocess\n",
    "import json\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------- Move this OUTSIDE the class ---------\n",
    "def process_file(args):\n",
    "\tfile_path, device_name, experiment_name, dataset_folder = args\n",
    "\treader = IoTPcapReader(dataset_folder)  # create lightweight reader in subprocess\n",
    "\tpackets = reader._run_tshark(file_path)\n",
    "\tdata = []\n",
    "\tfor packet in packets:\n",
    "\t\tif \"_source\" in packet and \"layers\" in packet[\"_source\"]:\n",
    "\t\t\trow = {\n",
    "\t\t\t\t'Experiment Name': experiment_name,\n",
    "\t\t\t\t'Device Name': device_name,\n",
    "\t\t\t\t'Packet JSON': json.dumps(packet)\n",
    "\t\t\t}\n",
    "\t\t\tdata.append(row)\n",
    "\treturn pd.DataFrame(data)\n",
    "# -----------------------------------------------\n",
    "\n",
    "class IoTPcapReader:\n",
    "\tdef __init__(self, dataset_folder: str):\n",
    "\t\tself.dataset_folder = dataset_folder\n",
    "\t\tself.global_dataframe = pd.DataFrame()\n",
    "\n",
    "\tdef _run_tshark(self, file_path: str) -> list:\n",
    "\t\ttry:\n",
    "\t\t\tcmd = [\"tshark\", \"-r\", file_path, \"-T\", \"json\"]\n",
    "\t\t\tresult = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "\t\t\tpackets = json.loads(result.stdout)\n",
    "\t\t\treturn packets\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error running tshark on {file_path}: {e}\")\n",
    "\t\t\treturn []\n",
    "\n",
    "\tdef read_all_pcap_files(self, max_workers=32):\n",
    "\t\ttasks = []\n",
    "\t\tfor experiment_folder in os.listdir(self.dataset_folder):\n",
    "\t\t\texperiment_path = os.path.join(self.dataset_folder, experiment_folder)\n",
    "\t\t\tif os.path.isdir(experiment_path):\n",
    "\t\t\t\tfor device_folder in os.listdir(experiment_path):\n",
    "\t\t\t\t\tdevice_path = os.path.join(experiment_path, device_folder)\n",
    "\t\t\t\t\tif os.path.isdir(device_path):\n",
    "\t\t\t\t\t\tfor inner_folder in os.listdir(device_path):\n",
    "\t\t\t\t\t\t\tinner_path = os.path.join(device_path, inner_folder)\n",
    "\t\t\t\t\t\t\tif os.path.isdir(inner_path):\n",
    "\t\t\t\t\t\t\t\tfor file_name in os.listdir(inner_path):\n",
    "\t\t\t\t\t\t\t\t\tif file_name.endswith('.pcap'):\n",
    "\t\t\t\t\t\t\t\t\t\tfile_path = os.path.join(inner_path, file_name)\n",
    "\t\t\t\t\t\t\t\t\t\ttasks.append((file_path, device_folder, experiment_folder, self.dataset_folder))\n",
    "\n",
    "\t\tprint(f\"Total .pcap files found: {len(tasks)}\")\n",
    "\n",
    "\t\tall_dfs = []\n",
    "\t\twith concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "\t\t\tfutures = [executor.submit(process_file, task) for task in tasks]\n",
    "\t\t\tfor future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing PCAP files\"):\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tdf_part = future.result()\n",
    "\t\t\t\t\tall_dfs.append(df_part)\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tprint(f\"⚠ Error processing file: {e}\")\n",
    "\n",
    "\t\tif all_dfs:\n",
    "\t\t\tself.global_dataframe = pd.concat(all_dfs, ignore_index=True)\n",
    "\t\telse:\n",
    "\t\t\tprint(\"⚠ No dataframes were generated — check your files.\")\n",
    "\n",
    "\t\tprint(f\"✅ Completed reading. Total packets collected: {len(self.global_dataframe)}\")\n",
    "\n",
    "\tdef save_as_pickle(self, output_file: str):\n",
    "\t\twith open(output_file, 'wb') as f:\n",
    "\t\t\tpickle.dump(self.global_dataframe, f)\n",
    "\t\tprint(f\"✅ Dataset saved as pickle at {output_file}\")\n",
    "\n",
    "\tdef load_pickle(self, pickle_file: str):\n",
    "\t\twith open(pickle_file, 'rb') as f:\n",
    "\t\t\tself.global_dataframe = pickle.load(f)\n",
    "\t\tprint(f\"✅ Dataset loaded from pickle at {pickle_file}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tdataset_folder_path = '../../Active-Experiments'\n",
    "\toutput_pickle_path = 'output_dataset.pkl'\n",
    "\n",
    "\tiot_reader = IoTPcapReader(dataset_folder_path)\n",
    "# \tiot_reader.read_all_pcap_files(max_workers=32)\n",
    "# \tiot_reader.save_as_pickle(output_pickle_path)\n",
    "\n",
    "\t# Load and check the pickle file\n",
    "\tiot_reader.load_pickle(output_pickle_path)\n",
    "\tprint(iot_reader.global_dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2056fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"_index\": \"packets-2025-05-01\", \"_type\": \"pcap_file\", \"_score\": null, \"_source\": {\"layers\": {\"frame\": {\"frame.encap_type\": \"1\", \"frame.time\": \"Apr 25, 2025 11:50:44.071024000 CEST\", \"frame.offset_shift\": \"0.000000000\", \"frame.time_epoch\": \"1745574644.071024000\", \"frame.time_delta\": \"0.000000000\", \"frame.time_delta_displayed\": \"0.000000000\", \"frame.time_relative\": \"0.000000000\", \"frame.number\": \"1\", \"frame.len\": \"85\", \"frame.cap_len\": \"85\", \"frame.marked\": \"0\", \"frame.ignored\": \"0\", \"frame.protocols\": \"eth:ethertype:ip:tcp:ssl\"}, \"eth\": {\"eth.dst\": \"d8:9e:f3:39:89:bf\", \"eth.dst_tree\": {\"eth.dst_resolved\": \"Dell_39:89:bf\", \"eth.addr\": \"d8:9e:f3:39:89:bf\", \"eth.addr_resolved\": \"Dell_39:89:bf\", \"eth.lg\": \"0\", \"eth.ig\": \"0\"}, \"eth.src\": \"48:e7:da:cd:de:a1\", \"eth.src_tree\": {\"eth.src_resolved\": \"48:e7:da:cd:de:a1\", \"eth.addr\": \"48:e7:da:cd:de:a1\", \"eth.addr_resolved\": \"48:e7:da:cd:de:a1\", \"eth.lg\": \"0\", \"eth.ig\": \"0\"}, \"eth.type\": \"0x00000800\"}, \"ip\": {\"ip.version\": \"4\", \"ip.hdr_len\": \"20\", \"ip.dsfield\": \"0x00000000\", \"ip.dsfield_tree\": {\"ip.dsfield.dscp\": \"0\", \"ip.dsfield.ecn\": \"0\"}, \"ip.len\": \"71\", \"ip.id\": \"0x00000043\", \"ip.flags\": \"0x00000000\", \"ip.flags_tree\": {\"ip.flags.rb\": \"0\", \"ip.flags.df\": \"0\", \"ip.flags.mf\": \"0\", \"ip.frag_offset\": \"0\"}, \"ip.ttl\": \"255\", \"ip.proto\": \"6\", \"ip.checksum\": \"0x0000db06\", \"ip.checksum.status\": \"2\", \"ip.src\": \"192.168.10.117\", \"ip.addr\": \"34.226.242.103\", \"ip.src_host\": \"192.168.10.117\", \"ip.host\": \"34.226.242.103\", \"ip.dst\": \"34.226.242.103\", \"ip.dst_host\": \"34.226.242.103\"}, \"tcp\": {\"tcp.srcport\": \"59293\", \"tcp.dstport\": \"8883\", \"tcp.port\": \"8883\", \"tcp.stream\": \"0\", \"tcp.len\": \"31\", \"tcp.seq\": \"1\", \"tcp.nxtseq\": \"32\", \"tcp.ack\": \"1\", \"tcp.hdr_len\": \"20\", \"tcp.flags\": \"0x00000018\", \"tcp.flags_tree\": {\"tcp.flags.res\": \"0\", \"tcp.flags.ns\": \"0\", \"tcp.flags.cwr\": \"0\", \"tcp.flags.ecn\": \"0\", \"tcp.flags.urg\": \"0\", \"tcp.flags.ack\": \"1\", \"tcp.flags.push\": \"1\", \"tcp.flags.reset\": \"0\", \"tcp.flags.syn\": \"0\", \"tcp.flags.fin\": \"0\", \"tcp.flags.str\": \"\\\\u00c2\\\\u00b7\\\\u00c2\\\\u00b7\\\\u00c2\\\\u00b7\\\\u00c2\\\\u00b7\\\\u00c2\\\\u00b7\\\\u00c2\\\\u00b7\\\\u00c2\\\\u00b7AP\\\\u00c2\\\\u00b7\\\\u00c2\\\\u00b7\\\\u00c2\\\\u00b7\"}, \"tcp.window_size_value\": \"13226\", \"tcp.window_size\": \"13226\", \"tcp.window_size_scalefactor\": \"-1\", \"tcp.checksum\": \"0x0000e6e7\", \"tcp.checksum.status\": \"2\", \"tcp.urgent_pointer\": \"0\", \"tcp.analysis\": {\"tcp.analysis.bytes_in_flight\": \"31\", \"tcp.analysis.push_bytes_sent\": \"31\"}, \"Timestamps\": {\"tcp.time_relative\": \"0.000000000\", \"tcp.time_delta\": \"0.000000000\"}, \"tcp.payload\": \"17:03:03:00:1a:00:00:00:00:00:00:00:19:ca:a5:a1:b6:db:07:94:35:66:1a:3a:41:af:aa:ec:7c:8a:8c\"}, \"ssl\": {\"ssl.record\": {\"ssl.record.content_type\": \"23\", \"ssl.record.version\": \"0x00000303\", \"ssl.record.length\": \"26\", \"ssl.app_data\": \"00:00:00:00:00:00:00:19:ca:a5:a1:b6:db:07:94:35:66:1a:3a:41:af:aa:ec:7c:8a:8c\"}}}}}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iot_reader.global_dataframe[\"Packet JSON\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98657351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "from collections import defaultdict\n",
    "\n",
    "class DNSPacketAnalyzer:\n",
    "\tdef __init__(self, global_dataframe: pd.DataFrame, output_folder: str = './dns_analysis_plots'):\n",
    "\t\tself.df = global_dataframe\n",
    "\t\tself.output_folder = output_folder\n",
    "\t\tos.makedirs(self.output_folder, exist_ok=True)\n",
    "\t\tself.preprocessed_df = None\n",
    "\n",
    "\tdef preprocess_dns_packets(self):\n",
    "\t\tdns_records = []\n",
    "\n",
    "\t\tfor _, row in self.df.iterrows():\n",
    "\t\t\tpacket_json = json.loads(row.get('Packet JSON', '{}'))\n",
    "\t\t\tsource = packet_json.get('_source', {})\n",
    "\t\t\tlayers = source.get('layers', {})\n",
    "\t\t\tframe = layers.get('frame', {})\n",
    "\t\t\tdns = layers.get('dns', {})\n",
    "\t\t\tdns_flags = dns.get('dns.flags_tree', {})\n",
    "\n",
    "\t\t\tprotocols = frame.get('frame.protocols', '')\n",
    "\t\t\tis_dns = 'dns' in protocols\n",
    "\n",
    "\t\t\tif is_dns:\n",
    "\t\t\t\trecord = {\n",
    "\t\t\t\t\t'Device Name': row['Device Name'],\n",
    "\t\t\t\t\t'frame_time': frame.get('frame.time', ''),\n",
    "\t\t\t\t\t'frame_len': int(frame.get('frame.len', 0)) if frame.get('frame.len', '').isdigit() else 0,\n",
    "\t\t\t\t\t'protocols': protocols,\n",
    "\t\t\t\t\t'is_response': dns_flags.get('dns.flags.response', '') == '1',\n",
    "\t\t\t\t\t'queries': dns.get('Queries', {}),\n",
    "\t\t\t\t\t'answers': dns.get('Answers', {}),\n",
    "\t\t\t\t\t'edns': dns.get('dns.opt', ''),\n",
    "\t\t\t\t}\n",
    "\n",
    "\t\t\t\trecord['ttl_list'] = [int(a.get('dns.resp.ttl', 0)) for a in record['answers'].values()] if record['answers'] else []\n",
    "\t\t\t\trecord['query_types'] = [q.get('dns.qry.type', '') for q in record['queries'].values()] if record['queries'] else []\n",
    "\t\t\t\trecord['query_names'] = [q.get('dns.qry.name', '') for q in record['queries'].values()] if record['queries'] else []\n",
    "\n",
    "\t\t\t\tdns_records.append(record)\n",
    "\n",
    "\t\tself.preprocessed_df = pd.DataFrame(dns_records)\n",
    "\t\tprint(f\"Preprocessed {len(self.preprocessed_df)} DNS packets.\")\n",
    "\n",
    "\tdef plot_bar(self, df, x, y, title, ylabel, filename, log_scale=False):\n",
    "\t\tplt.figure(figsize=(3.33, 2.2))\n",
    "\t\tplt.bar(df[x], df[y], width=0.6)\n",
    "\t\tif log_scale:\n",
    "\t\t\tplt.yscale('log')\n",
    "\t\tplt.xlabel(x, fontsize=7)\n",
    "\t\tplt.ylabel(ylabel, fontsize=7)\n",
    "# \t\tplt.title(title, fontsize=7)\n",
    "\t\tplt.xticks(rotation=45, ha='right', fontsize=5.5)\n",
    "\t\tplt.yticks(fontsize=6)\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.savefig(os.path.join(self.output_folder, filename), bbox_inches='tight', dpi=300)\n",
    "\t\tplt.close()\n",
    "\t\t\n",
    "\t\t# Save DataFrame + metadata as pickle\n",
    "\t\tpickle_data = {\n",
    "\t\t\t\"df\": df,\n",
    "\t\t\t\"x\": x,\n",
    "\t\t\t\"y\": y\n",
    "\t\t}\n",
    "\t\tpickle_path = os.path.join(self.output_folder, filename) + '.pkl'\n",
    "\t\twith open(pickle_path, 'wb') as f:\n",
    "\t\t\tpickle.dump(pickle_data, f)\n",
    "\n",
    "\tdef plot_dns_query_counts(self):\n",
    "\t\tsummary = self.preprocessed_df.groupby('Device Name').size().reset_index(name='Number of Queries')\n",
    "\t\tself.plot_bar(summary, 'Device Name', 'Number of Queries', 'DNS Queries per Device', 'Queries', 'dns_query_counts.pdf')\n",
    "\n",
    "\tdef plot_average_ttl(self):\n",
    "\t\texploded = self.preprocessed_df.explode('ttl_list')\n",
    "\t\texploded = exploded[exploded['ttl_list'] > 0]\n",
    "\t\tsummary = exploded.groupby('Device Name')['ttl_list'].mean().reset_index()\n",
    "\t\tself.plot_bar(summary, 'Device Name', 'ttl_list', 'Average TTL per Device', 'Avg. TTL (log, s)', 'average_ttl_log.pdf', log_scale=True)\n",
    "\n",
    "\tdef plot_dns_answer_counts(self):\n",
    "\t\tsummary = self.preprocessed_df.explode('answers').groupby('Device Name').size().reset_index(name='Number of Answers')\n",
    "\t\tself.plot_bar(summary, 'Device Name', 'Number of Answers', 'DNS Answers per Device', 'DNS Answers', 'dns_answer_counts.pdf')\n",
    "\n",
    "\tdef plot_dns_query_types(self):\n",
    "\t\tquery_types = self.preprocessed_df.explode('query_names')\n",
    "\t\tcounts = query_types.groupby(['Device Name', 'query_names']).size().reset_index(name='Count')\n",
    "\t\tpivot = counts.pivot(index='Device Name', columns='query_names', values='Count').fillna(0)\n",
    "\t\tax = pivot.plot(kind='bar', stacked=True, figsize=(3.33, 2.5), width=0.6)\n",
    "\t\tplt.xlabel('Device Name', fontsize=7)\n",
    "\t\tplt.ylabel('Query Count', fontsize=7)\n",
    "# \t\tplt.title('DNS Query Types per Device', fontsize=7)\n",
    "\t\tplt.xticks(rotation=45, ha='right', fontsize=6)\n",
    "\t\tplt.yticks(fontsize=6)\n",
    "\t\tplt.legend(fontsize=5, loc='upper right', frameon=False)\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.savefig(os.path.join(self.output_folder, 'dns_query_types.pdf'), bbox_inches='tight', dpi=300)\n",
    "\t\tplt.close()\n",
    "\n",
    "\tdef plot_avg_time_between_queries(self):\n",
    "\t\ttimes = self.preprocessed_df[~self.preprocessed_df['is_response']]\n",
    "\t\ttimes['parsed_time'] = pd.to_datetime(times['frame_time'], errors='coerce')\n",
    "\t\ttimes = times.dropna(subset=['parsed_time'])\n",
    "\t\tavg_times = times.sort_values('parsed_time').groupby('Device Name')['parsed_time'].apply(lambda x: x.diff().mean().total_seconds() if len(x) > 1 else 0).reset_index(name='Avg Time Between Queries')\n",
    "\t\tself.plot_bar(avg_times, 'Device Name', 'Avg Time Between Queries', 'Avg. Time Between DNS Queries', 'Avg. Time (log, s)', 'avg_time_between_queries_log.pdf', log_scale=True)\n",
    "\n",
    "\tdef plot_distinct_addresses(self):\n",
    "\t\taddr_df = self.preprocessed_df.explode('answers')\n",
    "\t\taddr_df['dns_a'] = addr_df['answers'].apply(lambda a: a.get('dns.a') if isinstance(a, dict) else None)\n",
    "\t\tdistinct_counts = addr_df.dropna(subset=['dns_a']).groupby('Device Name')['dns_a'].nunique().reset_index(name='Distinct Addresses')\n",
    "\t\tself.plot_bar(distinct_counts, 'Device Name', 'Distinct Addresses', 'Distinct DNS Addresses per Device', 'Distinct Addr.', 'distinct_addresses.pdf')\n",
    "\n",
    "\tdef plot_avg_answers_per_frame(self):\n",
    "\t\tans_df = self.preprocessed_df.copy()\n",
    "\t\tans_df['answer_count'] = ans_df['answers'].apply(lambda a: len(a) if isinstance(a, dict) else 0)\n",
    "\t\tavg_ans = ans_df.groupby('Device Name')['answer_count'].mean().reset_index(name='Answers per Frame')\n",
    "\t\tself.plot_bar(avg_ans, 'Device Name', 'Answers per Frame', 'Avg. DNS Answers per Frame', 'Avg. Answers / Frame', 'average_answers_per_frame.pdf')\n",
    "\n",
    "\tdef calculate_ipv6_query_percentage(self):\n",
    "\t\tipv6_df = self.preprocessed_df.explode('query_types')\n",
    "\t\ttotal_counts = ipv6_df.groupby('Device Name').size()\n",
    "\t\tipv6_counts = ipv6_df[ipv6_df['query_types'] == '28'].groupby('Device Name').size()\n",
    "\t\tpercent_df = (ipv6_counts / total_counts * 100).fillna(0).reset_index(name='IPv6 Query Percentage')\n",
    "\t\tself.plot_bar(percent_df, 'Device Name', 'IPv6 Query Percentage', 'IPv6 Queries per Device', 'IPv6 Query %', 'ipv6_query_percentage.pdf')\n",
    "\n",
    "\tdef calculate_average_retries(self):\n",
    "\t\tretries_df = self.preprocessed_df.explode('query_names')\n",
    "\t\tretries_count = retries_df.groupby(['Device Name', 'query_names']).size().reset_index(name='count')\n",
    "\t\tavg_retries = retries_count[retries_count['count'] > 1].groupby('Device Name')['count'].mean().reset_index(name='Average Retries')\n",
    "\t\tself.plot_bar(avg_retries, 'Device Name', 'Average Retries', 'Avg. DNS Query Retries per Device', 'Avg. Retries', 'average_dns_retries.pdf')\n",
    "\n",
    "\tdef plot_query_rate(self):\n",
    "\t\ttimes = self.preprocessed_df[~self.preprocessed_df['is_response']]\n",
    "\t\ttimes['parsed_time'] = pd.to_datetime(times['frame_time'], errors='coerce')\n",
    "\t\trate_df = times.dropna(subset=['parsed_time']).groupby('Device Name').apply(lambda x: len(x) / (x['parsed_time'].max() - x['parsed_time'].min()).total_seconds() if len(x) > 1 else 0).reset_index(name='Query Rate (queries/sec)')\n",
    "\t\tself.plot_bar(rate_df, 'Device Name', 'Query Rate (queries/sec)', 'DNS Query Rate per Device', 'Queries/sec', 'query_rate.pdf')\n",
    "\n",
    "\tdef plot_protocol_distribution(self):\n",
    "\t\tproto_counts = defaultdict(lambda: defaultdict(int))\n",
    "\t\tfor _, row in self.df.iterrows():\n",
    "\t\t\tdevice = row['Device Name']\n",
    "\t\t\tpacket_json = json.loads(row.get('Packet JSON', '{}'))\n",
    "\t\t\tprotocols = packet_json.get('_source', {}).get('layers', {}).get('frame', {}).get('frame.protocols', '')\n",
    "\t\t\tfor proto in protocols.split(':'):\n",
    "\t\t\t\tproto_counts[device][proto] += 1\n",
    "\t\tproto_df = pd.DataFrame(proto_counts).fillna(0).T\n",
    "\t\tax = proto_df.plot(kind='bar', stacked=True, figsize=(3.33, 2.5), width=0.6)\n",
    "\t\tplt.xlabel('Device Name', fontsize=7)\n",
    "\t\tplt.ylabel('Packet Count', fontsize=7)\n",
    "\t\tplt.yscale('log')\n",
    "# \t\tplt.title('Protocol Distribution per Device', fontsize=7)\n",
    "\t\tplt.xticks(rotation=45, ha='right', fontsize=6)\n",
    "\t\tplt.yticks(fontsize=6)\n",
    "\t\tplt.legend(title='Protocols', fontsize=5, title_fontsize=6, bbox_to_anchor=(1.05, 1), ncols=4, frameon=False)\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.savefig(os.path.join(self.output_folder, 'protocol_distribution.pdf'), bbox_inches='tight', dpi=300)\n",
    "\t\tplt.close()\n",
    "\n",
    "\tdef plot_mdns_count(self):\n",
    "\t\tmdns_df = self.preprocessed_df[self.preprocessed_df['protocols'].str.lower().str.contains('mdns')]\n",
    "\t\tsummary = mdns_df.groupby('Device Name').size().reset_index(name='MDNS Count')\n",
    "\t\tself.plot_bar(summary, 'Device Name', 'MDNS Count', 'MDNS Packet Count per Device', 'MDNS Packets', 'mdns_count.pdf')\n",
    "\n",
    "\tdef analyze_dns_query_context(self, time_window=5):\n",
    "\t\tself.df['Timestamp'] = self.df['Packet JSON'].apply(\n",
    "\t\t\tlambda x: parser.parse(json.loads(x).get('_source', {}).get('layers', {}).get('frame', {}).get('frame.time', '')) if x else None)\n",
    "\t\tself.preprocessed_df['Timestamp'] = self.preprocessed_df['frame_time'].apply(\n",
    "\t\t\tlambda x: parser.parse(x) if x else None)\n",
    "\n",
    "\t\tquery_contexts = []\n",
    "\t\ttraffic_context_count = {}\n",
    "\n",
    "\t\tfor _, dns_row in self.preprocessed_df.iterrows():\n",
    "\t\t\tdns_time = dns_row['Timestamp']\n",
    "\t\t\tdevice_name = dns_row['Device Name']\n",
    "\t\t\tdns_query = dns_row['queries']\n",
    "\n",
    "\t\t\tif dns_query:\n",
    "\t\t\t\tdns_query_name = list(dns_query.values())[0].get('dns.qry.name', '')\n",
    "\n",
    "\t\t\t\trelated_packets = self.df[\n",
    "\t\t\t\t\t(self.df['Device Name'] == device_name) &\n",
    "\t\t\t\t\t(self.df['Timestamp'] >= dns_time - pd.Timedelta(seconds=time_window)) &\n",
    "\t\t\t\t\t(self.df['Timestamp'] <= dns_time + pd.Timedelta(seconds=time_window))\n",
    "\t\t\t\t]\n",
    "\n",
    "\t\t\t\treasons = []\n",
    "\n",
    "\t\t\t\tfor _, packet in related_packets.iterrows():\n",
    "\t\t\t\t\tpacket_json = json.loads(packet['Packet JSON'])\n",
    "\t\t\t\t\tprotocols = packet_json.get('_source', {}).get('layers', {}).get('frame', {}).get('frame.protocols', '')\n",
    "\n",
    "\t\t\t\t\tif 'tcp' in protocols and 'http' in protocols:\n",
    "\t\t\t\t\t\treasons.append(\"HTTP Request after DNS\")\n",
    "\t\t\t\t\telif 'tcp' in protocols and 'tcp.analysis.retransmission' in packet_json.get('_source', {}).get('layers', {}).get('tcp', {}):\n",
    "\t\t\t\t\t\treasons.append(\"TCP Retransmission before DNS\")\n",
    "\t\t\t\t\telif 'tcp' in protocols and 'tcp.flags.syn' in packet_json.get('_source', {}).get('layers', {}).get('tcp', {}):\n",
    "\t\t\t\t\t\treasons.append(\"TCP SYN before DNS\")\n",
    "\t\t\t\t\telif 'dhcp' in protocols:\n",
    "\t\t\t\t\t\treasons.append(\"DHCP before DNS\")\n",
    "\t\t\t\t\telif 'icmp' in protocols:\n",
    "\t\t\t\t\t\treasons.append(\"ICMP before DNS\")\n",
    "\t\t\t\t\telif 'quic' in protocols:\n",
    "\t\t\t\t\t\treasons.append(\"QUIC traffic near DNS\")\n",
    "\n",
    "\t\t\t\treasons = list(set(reasons))  # Remove duplicates\n",
    "\n",
    "\t\t\t\tquery_contexts.append({\n",
    "\t\t\t\t\t'Device Name': device_name,\n",
    "\t\t\t\t\t'DNS Query': dns_query_name,\n",
    "\t\t\t\t\t'Query Time': dns_time,\n",
    "\t\t\t\t\t'Traffic Context': ', '.join(reasons) if reasons else \"Unknown\"\n",
    "\t\t\t\t})\n",
    "\n",
    "\t\t\t\tfor reason in reasons:\n",
    "\t\t\t\t\tif device_name not in traffic_context_count:\n",
    "\t\t\t\t\t\ttraffic_context_count[device_name] = {}\n",
    "\t\t\t\t\tif reason not in traffic_context_count[device_name]:\n",
    "\t\t\t\t\t\ttraffic_context_count[device_name][reason] = 0\n",
    "\t\t\t\t\ttraffic_context_count[device_name][reason] += 1\n",
    "\n",
    "\t\tquery_context_df = pd.DataFrame(query_contexts)\n",
    "\t\tquery_context_df.to_csv(os.path.join(self.output_folder, 'dns_query_context.csv'), index=False)\n",
    "\t\tprint(\"DNS query context analysis completed. Results saved in 'dns_query_context.csv'.\")\n",
    "\n",
    "\t\tself.plot_traffic_context_distribution(traffic_context_count)\n",
    "\n",
    "\t\treturn query_context_df\n",
    "\t\n",
    "\tdef plot_dns_query_counts_normalized(self):\n",
    "\t\ttimes = self.preprocessed_df.copy()\n",
    "\t\ttimes['parsed_time'] = pd.to_datetime(times['frame_time'], errors='coerce')\n",
    "\t\ttimes = times.dropna(subset=['parsed_time'])\n",
    "\t\tdevice_durations = times.groupby('Device Name')['parsed_time'].agg(['min', 'max'])\n",
    "\t\tdevice_durations['duration_seconds'] = (device_durations['max'] - device_durations['min']).dt.total_seconds().clip(lower=1)\n",
    "\n",
    "\t\tquery_counts = self.preprocessed_df.groupby('Device Name').size().reset_index(name='Total Queries')\n",
    "\t\tquery_counts = query_counts.merge(device_durations['duration_seconds'], left_on='Device Name', right_index=True)\n",
    "\t\tquery_counts['Queries per Sec'] = query_counts['Total Queries'] / query_counts['duration_seconds']\n",
    "\t\tself.plot_bar(query_counts, 'Device Name', 'Queries per Sec', 'Normalized DNS Queries per Device', 'Queries/sec', 'dns_query_counts_normalized.pdf', log_scale=True)\n",
    "\n",
    "\tdef calculate_average_retries_normalized(self):\n",
    "\t\tretries_df = self.preprocessed_df.explode('query_names')\n",
    "\t\tretries_df['parsed_time'] = pd.to_datetime(retries_df['frame_time'], errors='coerce')\n",
    "\t\tretries_df = retries_df.dropna(subset=['parsed_time'])\n",
    "\t\tdevice_durations = retries_df.groupby('Device Name')['parsed_time'].agg(['min', 'max'])\n",
    "\t\tdevice_durations['duration_seconds'] = (device_durations['max'] - device_durations['min']).dt.total_seconds().clip(lower=1)\n",
    "\n",
    "\t\tretries_count = retries_df.groupby(['Device Name', 'query_names']).size().reset_index(name='count')\n",
    "\t\tavg_retries = retries_count[retries_count['count'] > 1].groupby('Device Name')['count'].mean().reset_index(name='Avg Retries >1')\n",
    "\t\tavg_retries = avg_retries.merge(device_durations['duration_seconds'], left_on='Device Name', right_index=True)\n",
    "\t\tavg_retries['Retries per Sec'] = avg_retries['Avg Retries >1'] / avg_retries['duration_seconds']\n",
    "\t\tself.plot_bar(avg_retries, 'Device Name', 'Retries per Sec', 'Normalized Avg. DNS Query Retries', 'Retries/sec', 'average_dns_retries_normalized.pdf', log_scale=True)\n",
    "\n",
    "\tdef plot_traffic_context_distribution(self, traffic_context_count):\n",
    "\t\tmatplotlib.rcParams.update({'font.size': 7})  # ACM small font\n",
    "\t\tdata = []\n",
    "\t\tfor device, reasons in traffic_context_count.items():\n",
    "\t\t\tfor reason, count in reasons.items():\n",
    "\t\t\t\tdata.append({'Device Name': device, 'Traffic Context': reason, 'Count': count})\n",
    "\n",
    "\t\tdf = pd.DataFrame(data)\n",
    "\t\tpivot_df = df.pivot(index='Device Name', columns='Traffic Context', values='Count').fillna(0)\n",
    "\t\tax = pivot_df.plot(kind='bar', stacked=True, figsize=(3.33, 2.5), width=0.6)\n",
    "\t\tplt.xlabel('Device Name', fontsize=7)\n",
    "\t\tplt.ylabel('Context Count', fontsize=7)\n",
    "# \t\tplt.title('Traffic Contexts Around DNS Queries', fontsize=7)\n",
    "\t\tplt.xticks(rotation=45, ha='right', fontsize=5.5)\n",
    "\t\tplt.yticks(fontsize=6)\n",
    "\t\tplt.legend(fontsize=5, bbox_to_anchor=(0.5, 1.30), ncols=2, frameon=False)\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.savefig(os.path.join(self.output_folder, 'traffic_context_distribution.pdf'), bbox_inches='tight', dpi=300)\n",
    "\t\tplt.close()\n",
    "\t\tprint(\"Traffic context distribution plot saved.\")\n",
    "\n",
    "\tdef plot_edns0_usage(self):\n",
    "\t\tedns_df = self.preprocessed_df[self.preprocessed_df['edns'] != '']\n",
    "\t\tsummary = edns_df.groupby('Device Name').size().reset_index(name='EDNS(0) Count')\n",
    "\t\tself.plot_bar(summary, 'Device Name', 'EDNS(0) Count', 'EDNS(0) Usage per Device', 'EDNS(0) Count', 'edns0_usage.pdf')\n",
    "\n",
    "\tdef analyze(self):\n",
    "\t\tself.preprocess_dns_packets()\n",
    "\t\tif not self.preprocessed_df.empty:\n",
    "\t\t\tself.plot_dns_query_counts()\n",
    "# \t\t\tself.plot_dns_query_counts_normalized()\n",
    "# \t\t\tself.plot_average_ttl()\n",
    "# \t\t\tself.plot_dns_answer_counts()\n",
    "# \t\t\tself.plot_dns_query_types()\n",
    "# \t\t\tself.plot_avg_time_between_queries()\n",
    "# \t\t\tself.plot_distinct_addresses()\n",
    "# \t\t\tself.plot_avg_answers_per_frame()\n",
    "# \t\t\tself.calculate_ipv6_query_percentage()\n",
    "\t\t\tself.calculate_average_retries()\n",
    "# \t\t\tself.calculate_average_retries_normalized()\n",
    "# \t\t\tself.plot_query_rate()\n",
    "# \t\t\tself.analyze_dns_query_context()\n",
    "# \t\t\tself.plot_protocol_distribution()\n",
    "# \t\t\tself.plot_mdns_count()\n",
    "# \t\t\tself.plot_edns0_usage()\n",
    "\t\t\tprint(f\"All plots saved to {self.output_folder}\")\n",
    "\t\telse:\n",
    "\t\t\tprint(\"No DNS packets found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57ea8fcd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import pickle\n",
    "# import json\n",
    "# import concurrent.futures\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Assuming DNSPacketAnalyzer is imported or defined here (as you provided)\n",
    "\n",
    "# def run_analyzer_for_experiment(experiment_name, experiment_df, base_output_folder):\n",
    "# \t\"\"\"\n",
    "# \tRun DNSPacketAnalyzer for a single experiment.\n",
    "# \t\"\"\"\n",
    "# \toutput_folder = os.path.join(base_output_folder, experiment_name)\n",
    "# \tanalyzer = DNSPacketAnalyzer(experiment_df, output_folder=output_folder)\n",
    "# \tanalyzer.analyze()\n",
    "# \treturn f\"✅ Completed analysis for {experiment_name}\"\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "# \t# Paths\n",
    "# \tinput_pickle_path = 'output_dataset.pkl'\n",
    "# \tbase_output_folder = './dns_analysis_plots_active'\n",
    "\n",
    "# \t# Group dataframe by experiment\n",
    "# \texperiment_groups = {name: df for name, df in iot_reader.global_dataframe.groupby('Experiment Name')}\n",
    "# \tprint(f\"Total experiments found: {len(experiment_groups)}\")\n",
    "\n",
    "# \t# Prepare tasks, skipping already completed ones\n",
    "# \ttasks = []\n",
    "# \tfor experiment_name, experiment_df in experiment_groups.items():\n",
    "# \t\texperiment_output_folder = os.path.join(base_output_folder, experiment_name)\n",
    "# \t\texpected_result_file = os.path.join(experiment_output_folder, 'dns_query_counts.pdf')\n",
    "\n",
    "# # \t\tif os.path.exists(expected_result_file):\n",
    "# # \t\t\tprint(f\"⚠ Skipping {experiment_name} — results already exist.\")\n",
    "# # \t\telse:\n",
    "# \t\ttasks.append((experiment_name, experiment_df, base_output_folder))\n",
    "\n",
    "# \tprint(f\"\\nExperiments to analyze: {len(tasks)} (skipping {len(experiment_groups) - len(tasks)})\")\n",
    "\n",
    "\n",
    "# \t# Run analysis in parallel using 32 cores\n",
    "# \twith concurrent.futures.ProcessPoolExecutor(max_workers=32) as executor:\n",
    "# \t\tfutures = [executor.submit(run_analyzer_for_experiment, exp_name, exp_df, base_output_folder) \n",
    "# \t\t\t\t   for exp_name, exp_df, base_output_folder in tasks]\n",
    "\n",
    "# \t\tfor future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Analyzing Experiments\"):\n",
    "# \t\t\ttry:\n",
    "# \t\t\t\tresult = future.result()\n",
    "# \t\t\t\tprint(result)\n",
    "# \t\t\texcept Exception as e:\n",
    "# \t\t\t\tprint(f\"⚠ Error during analysis: {e}\")\n",
    "\n",
    "# \tprint(\"✅ All experiment analyses completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2675ea18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "# Re-importing necessary libraries after code execution environment reset\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define folders\n",
    "base_dirs = [\n",
    "\t'./dns_analysis_plots',\n",
    "\t'./dns_analysis_plots_doh',\n",
    "\t'./dns_analysis_plots_active'\n",
    "]\n",
    "\n",
    "# Plot settings (ACM-style format)\n",
    "plt.rcParams.update({\n",
    "\t\"figure.figsize\": (3.33, 2.2),\n",
    "\t\"font.size\": 7,\n",
    "\t\"axes.labelsize\": 7,\n",
    "\t\"xtick.labelsize\": 5.5,\n",
    "\t\"ytick.labelsize\": 6\n",
    "})\n",
    "\n",
    "# Initialize data containers\n",
    "aggregated_data = {\n",
    "\t\"Average Query Count\": {},\n",
    "\t\"Average DNS Retries\": {}\n",
    "}\n",
    "\n",
    "# Search recursively for relevant pickle files\n",
    "for base in base_dirs:\n",
    "\tif not os.path.isdir(base):\n",
    "\t\tcontinue\n",
    "\tfor root, dirs, files in os.walk(base):\n",
    "\t\tfor file in files:\n",
    "\t\t\tif file.endswith('.pkl') and ('dns_query_count' in file or 'average_dns_retries' in file):\n",
    "\t\t\t\twith open(os.path.join(root, file), 'rb') as f:\n",
    "\t\t\t\t\tdata = pickle.load(f)\n",
    "\t\t\t\t\tmetric = \"Average Query Count\" if 'dns_query_count' in file else \"Average DNS Retries\"\n",
    "\t\t\t\t\tdf = data[\"df\"]\n",
    "\t\t\t\t\tx, y = data[\"x\"], data[\"y\"]\n",
    "\t\t\t\t\tfiltered_df = df[df[x] != 0]\n",
    "\t\t\t\t\tavg_df = filtered_df.groupby(x)[y].max().reset_index()\n",
    "\t\t\t\t\tlabel = os.path.basename(root)\n",
    "\t\t\t\t\taggregated_data[metric][label] = avg_df\n",
    "\n",
    "# Extend the plotting function with:\n",
    "# - custom scenario name mapping (via dictionary)\n",
    "# - list of scenarios to exclude from the x-axis\n",
    "\n",
    "def plot_clustered_barplot_customized(metric1_name, metric2_name, metric1_data, metric2_data,\n",
    "\t\t\t\t\t\t\t\t\t  custom_labels=None, exclude_list=None, durations_map=None):\n",
    "\tif custom_labels is None:\n",
    "\t\tcustom_labels = {}  # default to no renaming\n",
    "\tif exclude_list is None:\n",
    "\t\texclude_list = []  # default to include all\n",
    "\n",
    "\tscenarios = sorted(set(metric1_data.keys()) & set(metric2_data.keys()))\n",
    "\tscenarios = [sc for sc in scenarios if sc not in exclude_list]\n",
    "\n",
    "\tif not scenarios:\n",
    "\t\treturn\n",
    "\n",
    "\t# Sort scenarios by order in custom_labels if present, else alphabetically\n",
    "\tdef scenario_sort_key(sc):\n",
    "\t\treturn list(custom_labels.keys()).index(sc) if sc in custom_labels else len(custom_labels) + ord(sc[0])\n",
    "\n",
    "\tscenarios.sort(key=scenario_sort_key)\n",
    "\tx = np.arange(len(scenarios))\n",
    "\twidth = 0.35\n",
    "\n",
    "\tmetric1_values = [metric1_data[sc].iloc[:, 1].mean() for sc in scenarios]\n",
    "\tmetric2_values = [metric2_data[sc].iloc[:, 1].mean() for sc in scenarios]\n",
    "\n",
    "\t# Apply custom labels\n",
    "\tx_labels = [custom_labels.get(sc, sc) for sc in scenarios]\n",
    "\n",
    "\t# Normalize counts using the corresponding duration\n",
    "\tmetric1_values = []\n",
    "\tmetric2_values = []\n",
    "\tfor sc in scenarios:\n",
    "\t\tlabel = custom_labels.get(sc, sc)\n",
    "\t\tdur = durations_map.get(label, 1)\n",
    "\t\tval1 = metric1_data.get(sc, pd.DataFrame({0: [0], 1: [0]})).iloc[:, 1].mean() / dur\n",
    "\t\tval2 = metric2_data.get(sc, pd.DataFrame({0: [0], 1: [0]})).iloc[:, 1].mean() / dur\n",
    "\t\tmetric1_values.append(val1)\n",
    "\t\tmetric2_values.append(val2)\n",
    "\n",
    "\tplt.figure(figsize=(3.33, 2.2))\n",
    "\tplt.bar(x - width/2, metric1_values, width, label=\"Average Queries\", hatch='///')\n",
    "\tplt.bar(x + width/2, metric2_values, width, label=\"Average Retries\", hatch='\\\\\\\\\\\\')\n",
    "\n",
    "\tplt.yscale('log')\n",
    "\tplt.xticks(x, x_labels, rotation=45, ha='right', fontsize=5.5)\n",
    "\tplt.ylabel(\"count/s\", fontsize=7)\n",
    "\tplt.legend(fontsize=6, loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2)\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(\"aggregated_clustered_barplot_customized.pdf\", bbox_inches='tight', dpi=300)\n",
    "\tplt.close()\n",
    "\n",
    "\tprint(\"Hello\")\n",
    "\n",
    "# Define mapping of custom labels to their respective experiment durations in seconds\n",
    "custom_label_durations_sec = {\n",
    "\t\"Benchmark (Do53)\": 3 * 3600 + 46 * 60,\n",
    "\t\"Forced to DoH\": 2 * 3600 + 53 * 60,\n",
    "\t\"Delete all RRs\": 1 * 3600 + 36 * 60,\n",
    "\t\"Fake A record\": 44 * 60,\n",
    "\t\"Fake AAAA record\": 28 * 60,\n",
    "\t\"Fake CNAME record\": 1 * 3600 + 32 * 60,\n",
    "\t\"Forged TTL ($0$s)\": 1 * 3600 + 55 * 60,\n",
    "\t\"Forged TTL ($0.1$s)\": 1 * 3600,\n",
    "\t\"Forged TTL ($1$s)\": 48 * 60,\n",
    "\t\"Forged TTL ($10^9$s)\": 1 * 3600 + 2 * 60,\n",
    "}\n",
    "\n",
    "# Example custom label mapping for known scenarios\n",
    "custom_label_map = {\n",
    "\t\"dns_analysis_plots\": \"Benchmark (Do53)\",\n",
    "\t\"dns_analysis_plots_doh\": \"Forced to DoH\",\n",
    "\t\"Experiment_19_example_com\": \"Delete all RRs\",\n",
    "\t\"Experiment_1_A_192_0_2_1\": \"Fake A record\",\n",
    "\t\"Experiment_1_AAAA_2001_0db8_1\": \"Fake AAAA record\",\n",
    "\t\"Experiment_1_CNAME_alias\": \"Fake CNAME record\",\n",
    "\t\"Experiment_1_ttl_0\": \"Forged TTL ($0$s)\",\n",
    "\t\"Experiment_1_ttl_01\": \"Forged TTL ($0.1$s)\",\n",
    "\t\"Experiment_1_ttl_0_1\": \"Forged TTL ($1$s)\",\n",
    "\t\"Experiment_1_ttl_01000000000\": \"Forged TTL ($10^9$s)\",\n",
    "}\n",
    "\n",
    "# Example exclude list (currently empty, can be extended)\n",
    "excluded_scenarios = [\"Experiment_24\", \"Experiment_25_0_New\", \"Experiment_25_512_New\", \"Experiment_26_100_False_New\", \"Experiment_26_1_False_New\"]\n",
    "\n",
    "# Execute customized plot\n",
    "plot_clustered_barplot_customized(\n",
    "\t\"Average Query Count\", \"Average DNS Retries\",\n",
    "\taggregated_data[\"Average Query Count\"],\n",
    "\taggregated_data[\"Average DNS Retries\"],\n",
    "\tcustom_labels=custom_label_map,\n",
    "\texclude_list=excluded_scenarios,\n",
    "\tdurations_map=custom_label_durations_sec\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
